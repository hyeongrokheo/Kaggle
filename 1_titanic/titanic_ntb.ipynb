{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d6bf2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d29735d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "gender = pd.read_csv('data/gender_submission.csv')\n",
    "\n",
    "train['Age'] = train['Age'].fillna(29.6)\n",
    "test['Age'] = test['Age'].fillna(29.6)\n",
    "\n",
    "train['Fare'] = train['Fare'].fillna(32.2)\n",
    "test['Fare'] = test['Fare'].fillna(32.2)\n",
    "\n",
    "train['Age'] = train['Age'] / 80\n",
    "test['Age'] = test['Age'] / 80\n",
    "\n",
    "train['Pclass'] = train['Pclass'] / 3\n",
    "test['Pclass'] = test['Pclass'] / 3\n",
    "\n",
    "train['SibSp'] = train['SibSp'] / 5\n",
    "test['SibSp'] = test['SibSp'] / 5\n",
    "\n",
    "train['Parch'] = train['Parch'] / 6\n",
    "test['Parch'] = test['Parch'] / 6\n",
    "\n",
    "train['Fare'] = train['Fare'] / 520\n",
    "test['Fare'] = test['Fare'] / 520\n",
    "\n",
    "train= pd.concat([train, pd.get_dummies(train['Sex'])], axis=1)\n",
    "test= pd.concat([test, pd.get_dummies(test['Sex'])], axis=1)\n",
    "\n",
    "train= pd.concat([train, pd.get_dummies(train['Embarked'])], axis=1)\n",
    "test= pd.concat([test, pd.get_dummies(test['Embarked'])], axis=1)\n",
    "\n",
    "\n",
    "mask = np.random.rand(len(train)) < 0.8\n",
    "train, valid = train[mask], train[~mask]\n",
    "# print(train)\n",
    "# print(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14f9d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, input_dim=10, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "checkpoint_path = \"my_checkpoint.ckpt\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4e8dbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "708/708 [==============================] - 0s 437us/step - loss: 0.5998 - acc: 0.6709 - val_loss: 0.5342 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53415, saving model to my_checkpoint.ckpt\n",
      "Epoch 2/100\n",
      "708/708 [==============================] - 0s 332us/step - loss: 0.5288 - acc: 0.8043 - val_loss: 0.5198 - val_acc: 0.8142\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53415 to 0.51980, saving model to my_checkpoint.ckpt\n",
      "Epoch 3/100\n",
      "708/708 [==============================] - 0s 330us/step - loss: 0.5111 - acc: 0.8160 - val_loss: 0.5094 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51980 to 0.50944, saving model to my_checkpoint.ckpt\n",
      "Epoch 4/100\n",
      "708/708 [==============================] - 0s 334us/step - loss: 0.5110 - acc: 0.7844 - val_loss: 0.5061 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.50944 to 0.50611, saving model to my_checkpoint.ckpt\n",
      "Epoch 5/100\n",
      "708/708 [==============================] - 0s 330us/step - loss: 0.5120 - acc: 0.8022 - val_loss: 0.4840 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50611 to 0.48400, saving model to my_checkpoint.ckpt\n",
      "Epoch 6/100\n",
      "708/708 [==============================] - 0s 332us/step - loss: 0.5073 - acc: 0.7760 - val_loss: 0.4857 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48400\n",
      "Epoch 7/100\n",
      "708/708 [==============================] - 0s 340us/step - loss: 0.4974 - acc: 0.7950 - val_loss: 0.4858 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48400\n",
      "Epoch 8/100\n",
      "708/708 [==============================] - 0s 338us/step - loss: 0.4525 - acc: 0.8259 - val_loss: 0.4649 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.48400 to 0.46486, saving model to my_checkpoint.ckpt\n",
      "Epoch 9/100\n",
      "708/708 [==============================] - 0s 342us/step - loss: 0.4445 - acc: 0.8159 - val_loss: 0.4685 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.46486\n",
      "Epoch 10/100\n",
      "708/708 [==============================] - 0s 338us/step - loss: 0.4372 - acc: 0.8367 - val_loss: 0.4595 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.46486 to 0.45945, saving model to my_checkpoint.ckpt\n",
      "Epoch 11/100\n",
      "708/708 [==============================] - 0s 344us/step - loss: 0.4448 - acc: 0.8224 - val_loss: 0.4651 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.45945\n",
      "Epoch 12/100\n",
      "708/708 [==============================] - 0s 333us/step - loss: 0.4225 - acc: 0.8365 - val_loss: 0.4544 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.45945 to 0.45444, saving model to my_checkpoint.ckpt\n",
      "Epoch 13/100\n",
      "708/708 [==============================] - 0s 343us/step - loss: 0.4337 - acc: 0.8387 - val_loss: 0.4650 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.45444\n",
      "Epoch 14/100\n",
      "708/708 [==============================] - 0s 352us/step - loss: 0.4231 - acc: 0.8265 - val_loss: 0.4471 - val_acc: 0.8142\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.45444 to 0.44713, saving model to my_checkpoint.ckpt\n",
      "Epoch 15/100\n",
      "708/708 [==============================] - 0s 333us/step - loss: 0.4384 - acc: 0.8339 - val_loss: 0.4498 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.44713\n",
      "Epoch 16/100\n",
      "708/708 [==============================] - 0s 347us/step - loss: 0.4134 - acc: 0.8550 - val_loss: 0.4514 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.44713\n",
      "Epoch 17/100\n",
      "708/708 [==============================] - 0s 339us/step - loss: 0.3933 - acc: 0.8518 - val_loss: 0.4484 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.44713\n",
      "Epoch 18/100\n",
      "708/708 [==============================] - 0s 347us/step - loss: 0.4044 - acc: 0.8426 - val_loss: 0.4580 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.44713\n",
      "Epoch 19/100\n",
      "708/708 [==============================] - 0s 349us/step - loss: 0.3955 - acc: 0.8466 - val_loss: 0.4532 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.44713\n",
      "Epoch 20/100\n",
      "708/708 [==============================] - 0s 339us/step - loss: 0.4072 - acc: 0.8390 - val_loss: 0.4463 - val_acc: 0.8142\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.44713 to 0.44634, saving model to my_checkpoint.ckpt\n",
      "Epoch 21/100\n",
      "708/708 [==============================] - 0s 358us/step - loss: 0.4244 - acc: 0.8284 - val_loss: 0.4481 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.44634\n",
      "Epoch 22/100\n",
      "708/708 [==============================] - 0s 343us/step - loss: 0.4076 - acc: 0.8449 - val_loss: 0.4495 - val_acc: 0.8142\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.44634\n",
      "Epoch 23/100\n",
      "708/708 [==============================] - 0s 343us/step - loss: 0.4234 - acc: 0.8037 - val_loss: 0.4548 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.44634\n",
      "Epoch 24/100\n",
      "708/708 [==============================] - 0s 341us/step - loss: 0.4245 - acc: 0.8423 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.44634\n",
      "Epoch 25/100\n",
      "708/708 [==============================] - 0s 336us/step - loss: 0.4228 - acc: 0.8092 - val_loss: 0.4529 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.44634\n",
      "Epoch 26/100\n",
      "708/708 [==============================] - 0s 335us/step - loss: 0.4399 - acc: 0.8108 - val_loss: 0.4502 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.44634\n",
      "Epoch 27/100\n",
      "708/708 [==============================] - 0s 342us/step - loss: 0.3944 - acc: 0.8423 - val_loss: 0.4566 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.44634\n",
      "Epoch 28/100\n",
      "708/708 [==============================] - 0s 350us/step - loss: 0.3924 - acc: 0.8504 - val_loss: 0.4495 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.44634\n",
      "Epoch 29/100\n",
      "708/708 [==============================] - 0s 334us/step - loss: 0.3664 - acc: 0.8596 - val_loss: 0.4537 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.44634\n",
      "Epoch 30/100\n",
      "708/708 [==============================] - 0s 343us/step - loss: 0.4043 - acc: 0.8534 - val_loss: 0.4712 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.44634\n",
      "Epoch 31/100\n",
      "708/708 [==============================] - 0s 342us/step - loss: 0.3916 - acc: 0.8319 - val_loss: 0.4459 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.44634 to 0.44595, saving model to my_checkpoint.ckpt\n",
      "Epoch 32/100\n",
      "708/708 [==============================] - 0s 335us/step - loss: 0.3872 - acc: 0.8542 - val_loss: 0.4648 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.44595\n",
      "Epoch 33/100\n",
      "708/708 [==============================] - 0s 332us/step - loss: 0.3711 - acc: 0.8559 - val_loss: 0.4729 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.44595\n",
      "Epoch 34/100\n",
      "708/708 [==============================] - 0s 345us/step - loss: 0.3752 - acc: 0.8500 - val_loss: 0.4597 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.44595\n",
      "Epoch 35/100\n",
      "708/708 [==============================] - 0s 337us/step - loss: 0.3516 - acc: 0.8781 - val_loss: 0.4579 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.44595\n",
      "Epoch 36/100\n",
      "708/708 [==============================] - 0s 333us/step - loss: 0.3948 - acc: 0.8428 - val_loss: 0.4717 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.44595\n",
      "Epoch 37/100\n",
      "708/708 [==============================] - 0s 353us/step - loss: 0.4212 - acc: 0.8166 - val_loss: 0.4583 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.44595\n",
      "Epoch 38/100\n",
      "708/708 [==============================] - 0s 337us/step - loss: 0.3969 - acc: 0.8548 - val_loss: 0.4797 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.44595\n",
      "Epoch 39/100\n",
      "708/708 [==============================] - 0s 335us/step - loss: 0.3714 - acc: 0.8582 - val_loss: 0.4980 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.44595\n",
      "Epoch 40/100\n",
      "708/708 [==============================] - 0s 353us/step - loss: 0.3956 - acc: 0.8455 - val_loss: 0.4723 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.44595\n",
      "Epoch 41/100\n",
      "708/708 [==============================] - 0s 335us/step - loss: 0.3677 - acc: 0.8574 - val_loss: 0.4646 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.44595\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708/708 [==============================] - 0s 346us/step - loss: 0.3774 - acc: 0.8662 - val_loss: 0.4870 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.44595\n",
      "Epoch 43/100\n",
      "708/708 [==============================] - 0s 328us/step - loss: 0.4087 - acc: 0.8278 - val_loss: 0.4692 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.44595\n",
      "Epoch 44/100\n",
      "708/708 [==============================] - 0s 339us/step - loss: 0.3711 - acc: 0.8511 - val_loss: 0.4816 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.44595\n",
      "Epoch 45/100\n",
      "708/708 [==============================] - 0s 338us/step - loss: 0.3942 - acc: 0.8457 - val_loss: 0.4816 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.44595\n",
      "Epoch 46/100\n",
      "708/708 [==============================] - 0s 329us/step - loss: 0.3926 - acc: 0.8402 - val_loss: 0.4822 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.44595\n",
      "Epoch 47/100\n",
      "708/708 [==============================] - 0s 330us/step - loss: 0.3865 - acc: 0.8517 - val_loss: 0.4872 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.44595\n",
      "Epoch 48/100\n",
      "708/708 [==============================] - 0s 333us/step - loss: 0.3910 - acc: 0.8537 - val_loss: 0.4857 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.44595\n",
      "Epoch 49/100\n",
      "708/708 [==============================] - 0s 330us/step - loss: 0.3931 - acc: 0.8308 - val_loss: 0.4892 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.44595\n",
      "Epoch 50/100\n",
      "708/708 [==============================] - 0s 333us/step - loss: 0.3874 - acc: 0.8476 - val_loss: 0.4821 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.44595\n",
      "Epoch 51/100\n",
      "708/708 [==============================] - 0s 330us/step - loss: 0.4096 - acc: 0.8382 - val_loss: 0.4721 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.44595\n",
      "Epoch 52/100\n",
      "708/708 [==============================] - 0s 340us/step - loss: 0.3373 - acc: 0.8674 - val_loss: 0.4838 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.44595\n",
      "Epoch 53/100\n",
      "708/708 [==============================] - 0s 331us/step - loss: 0.3677 - acc: 0.8688 - val_loss: 0.4767 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.44595\n",
      "Epoch 54/100\n",
      "708/708 [==============================] - 0s 338us/step - loss: 0.4001 - acc: 0.8487 - val_loss: 0.4820 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.44595\n",
      "Epoch 55/100\n",
      "708/708 [==============================] - 0s 337us/step - loss: 0.3646 - acc: 0.8619 - val_loss: 0.4890 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.44595\n",
      "Epoch 56/100\n",
      "708/708 [==============================] - 0s 330us/step - loss: 0.3940 - acc: 0.8287 - val_loss: 0.4861 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.44595\n",
      "Epoch 57/100\n",
      "708/708 [==============================] - 0s 346us/step - loss: 0.3575 - acc: 0.8513 - val_loss: 0.4905 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.44595\n",
      "Epoch 58/100\n",
      "708/708 [==============================] - 0s 335us/step - loss: 0.3633 - acc: 0.8568 - val_loss: 0.4858 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.44595\n",
      "Epoch 59/100\n",
      "708/708 [==============================] - 0s 332us/step - loss: 0.3752 - acc: 0.8667 - val_loss: 0.4970 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.44595\n",
      "Epoch 60/100\n",
      "708/708 [==============================] - 0s 332us/step - loss: 0.3650 - acc: 0.8674 - val_loss: 0.4852 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.44595\n",
      "Epoch 61/100\n",
      "708/708 [==============================] - 0s 339us/step - loss: 0.3591 - acc: 0.8613 - val_loss: 0.4822 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.44595\n",
      "Epoch 62/100\n",
      "708/708 [==============================] - 0s 339us/step - loss: 0.3442 - acc: 0.8646 - val_loss: 0.4978 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.44595\n",
      "Epoch 63/100\n",
      "708/708 [==============================] - 0s 339us/step - loss: 0.3993 - acc: 0.8225 - val_loss: 0.5049 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.44595\n",
      "Epoch 64/100\n",
      "708/708 [==============================] - 0s 339us/step - loss: 0.3530 - acc: 0.8736 - val_loss: 0.5074 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.44595\n",
      "Epoch 65/100\n",
      "708/708 [==============================] - 0s 339us/step - loss: 0.3737 - acc: 0.8388 - val_loss: 0.5205 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.44595\n",
      "Epoch 66/100\n",
      "708/708 [==============================] - 0s 336us/step - loss: 0.3515 - acc: 0.8666 - val_loss: 0.5107 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.44595\n",
      "Epoch 67/100\n",
      "708/708 [==============================] - 0s 338us/step - loss: 0.3433 - acc: 0.8609 - val_loss: 0.5248 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.44595\n",
      "Epoch 68/100\n",
      "708/708 [==============================] - 0s 344us/step - loss: 0.3452 - acc: 0.8536 - val_loss: 0.5086 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.44595\n",
      "Epoch 69/100\n",
      "708/708 [==============================] - 0s 337us/step - loss: 0.3781 - acc: 0.8401 - val_loss: 0.5316 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.44595\n",
      "Epoch 70/100\n",
      "708/708 [==============================] - 0s 336us/step - loss: 0.3313 - acc: 0.8782 - val_loss: 0.5089 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.44595\n",
      "Epoch 71/100\n",
      "708/708 [==============================] - 0s 336us/step - loss: 0.3857 - acc: 0.8434 - val_loss: 0.5018 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.44595\n",
      "Epoch 72/100\n",
      "708/708 [==============================] - 0s 334us/step - loss: 0.3297 - acc: 0.8728 - val_loss: 0.4988 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.44595\n",
      "Epoch 73/100\n",
      "708/708 [==============================] - 0s 336us/step - loss: 0.3486 - acc: 0.8462 - val_loss: 0.5024 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.44595\n",
      "Epoch 74/100\n",
      "708/708 [==============================] - 0s 336us/step - loss: 0.3488 - acc: 0.8746 - val_loss: 0.5234 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.44595\n",
      "Epoch 75/100\n",
      "708/708 [==============================] - 0s 337us/step - loss: 0.3618 - acc: 0.8491 - val_loss: 0.5075 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.44595\n",
      "Epoch 76/100\n",
      "708/708 [==============================] - 0s 336us/step - loss: 0.3560 - acc: 0.8467 - val_loss: 0.5108 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.44595\n",
      "Epoch 77/100\n",
      "708/708 [==============================] - 0s 335us/step - loss: 0.3483 - acc: 0.8578 - val_loss: 0.5341 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.44595\n",
      "Epoch 78/100\n",
      "708/708 [==============================] - 0s 337us/step - loss: 0.3260 - acc: 0.8756 - val_loss: 0.5168 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.44595\n",
      "Epoch 79/100\n",
      "708/708 [==============================] - 0s 334us/step - loss: 0.3743 - acc: 0.8506 - val_loss: 0.5240 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.44595\n",
      "Epoch 80/100\n",
      "708/708 [==============================] - 0s 336us/step - loss: 0.3675 - acc: 0.8507 - val_loss: 0.5106 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.44595\n",
      "Epoch 81/100\n",
      "708/708 [==============================] - 0s 335us/step - loss: 0.3826 - acc: 0.8500 - val_loss: 0.5113 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.44595\n",
      "Epoch 82/100\n",
      "708/708 [==============================] - 0s 337us/step - loss: 0.3481 - acc: 0.8477 - val_loss: 0.5258 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.44595\n",
      "Epoch 83/100\n",
      "708/708 [==============================] - 0s 337us/step - loss: 0.3506 - acc: 0.8676 - val_loss: 0.5119 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.44595\n",
      "Epoch 84/100\n",
      "708/708 [==============================] - 0s 334us/step - loss: 0.3385 - acc: 0.8684 - val_loss: 0.5253 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.44595\n",
      "Epoch 85/100\n",
      "708/708 [==============================] - 0s 337us/step - loss: 0.3270 - acc: 0.8777 - val_loss: 0.5424 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00085: val_loss did not improve from 0.44595\n",
      "Epoch 86/100\n",
      "708/708 [==============================] - 0s 329us/step - loss: 0.3844 - acc: 0.8509 - val_loss: 0.5417 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.44595\n",
      "Epoch 87/100\n",
      "708/708 [==============================] - 0s 327us/step - loss: 0.3531 - acc: 0.8726 - val_loss: 0.5108 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.44595\n",
      "Epoch 88/100\n",
      "708/708 [==============================] - 0s 326us/step - loss: 0.3530 - acc: 0.8579 - val_loss: 0.5144 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.44595\n",
      "Epoch 89/100\n",
      "708/708 [==============================] - 0s 326us/step - loss: 0.3267 - acc: 0.8679 - val_loss: 0.5173 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.44595\n",
      "Epoch 90/100\n",
      "708/708 [==============================] - 0s 328us/step - loss: 0.4052 - acc: 0.8414 - val_loss: 0.5130 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.44595\n",
      "Epoch 91/100\n",
      "708/708 [==============================] - 0s 328us/step - loss: 0.3655 - acc: 0.8532 - val_loss: 0.5281 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.44595\n",
      "Epoch 92/100\n",
      "708/708 [==============================] - 0s 328us/step - loss: 0.3597 - acc: 0.8640 - val_loss: 0.5136 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.44595\n",
      "Epoch 93/100\n",
      "708/708 [==============================] - 0s 326us/step - loss: 0.3449 - acc: 0.8678 - val_loss: 0.5003 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.44595\n",
      "Epoch 94/100\n",
      "708/708 [==============================] - 0s 329us/step - loss: 0.3462 - acc: 0.8550 - val_loss: 0.5166 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.44595\n",
      "Epoch 95/100\n",
      "708/708 [==============================] - 0s 329us/step - loss: 0.3683 - acc: 0.8693 - val_loss: 0.5316 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.44595\n",
      "Epoch 96/100\n",
      "708/708 [==============================] - 0s 329us/step - loss: 0.3214 - acc: 0.8804 - val_loss: 0.4971 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.44595\n",
      "Epoch 97/100\n",
      "708/708 [==============================] - 0s 329us/step - loss: 0.3519 - acc: 0.8722 - val_loss: 0.5036 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.44595\n",
      "Epoch 98/100\n",
      "708/708 [==============================] - 0s 330us/step - loss: 0.3362 - acc: 0.8568 - val_loss: 0.5319 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.44595\n",
      "Epoch 99/100\n",
      "708/708 [==============================] - 0s 331us/step - loss: 0.3084 - acc: 0.8795 - val_loss: 0.5282 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.44595\n",
      "Epoch 100/100\n",
      "708/708 [==============================] - 0s 328us/step - loss: 0.3640 - acc: 0.8476 - val_loss: 0.5281 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.44595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x11069c430>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_x = train[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'male', 'C', 'Q', 'S']]\n",
    "train_y = train['Survived']\n",
    "\n",
    "valid_x = valid[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'male', 'C', 'Q', 'S']]\n",
    "valid_y = valid['Survived']\n",
    "\n",
    "history = model.fit(\n",
    "    train_x, train_y, epochs=100,\n",
    "    validation_data=(valid_x, valid_y),\n",
    "    batch_size=1,\n",
    "    callbacks=[checkpoint])\n",
    "\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef0a720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 970us/step - loss: 24.5845 - acc: 0.4153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[24.58445930480957, 0.41530054807662964]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4bc9cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x111094fa0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1761b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.43125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015056</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.58750</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013462</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.77500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018630</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.33750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.27500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.023630</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.37000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015481</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.48750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209423</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.48125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013942</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.37000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015481</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.37000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.042997</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pclass      Age  SibSp     Parch      Fare  female  male  C  Q  S\n",
       "0    1.000000  0.43125    0.0  0.000000  0.015056       0     1  0  1  0\n",
       "1    1.000000  0.58750    0.2  0.000000  0.013462       1     0  0  0  1\n",
       "2    0.666667  0.77500    0.0  0.000000  0.018630       0     1  0  1  0\n",
       "3    1.000000  0.33750    0.0  0.000000  0.016659       0     1  0  0  1\n",
       "4    1.000000  0.27500    0.2  0.166667  0.023630       1     0  0  0  1\n",
       "..        ...      ...    ...       ...       ...     ...   ... .. .. ..\n",
       "413  1.000000  0.37000    0.0  0.000000  0.015481       0     1  0  0  1\n",
       "414  0.333333  0.48750    0.0  0.000000  0.209423       1     0  1  0  0\n",
       "415  1.000000  0.48125    0.0  0.000000  0.013942       0     1  0  0  1\n",
       "416  1.000000  0.37000    0.0  0.000000  0.015481       0     1  0  0  1\n",
       "417  1.000000  0.37000    0.2  0.166667  0.042997       0     1  1  0  0\n",
       "\n",
       "[418 rows x 10 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test['Predict'] = model.predict(test[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'male', 'C', 'Q', 'S']])\n",
    "model.predict(test[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'male', 'C', 'Q', 'S']])\n",
    "test[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'male', 'C', 'Q', 'S']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32fcdb77",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Predict'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/Algorithm/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3079\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3080\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Predict'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-086045dfedd6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Predict'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m0.5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'Survived'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Predict'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0;36m0.5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'Survived'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Survived'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Survived'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/Algorithm/lib/python3.8/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3022\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3023\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3024\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3025\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3026\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/Algorithm/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3080\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3082\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3084\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtolerance\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Predict'"
     ]
    }
   ],
   "source": [
    "test.loc[test['Predict'] < 0.5, 'Survived'] = 0\n",
    "test.loc[test['Predict'] >= 0.5, 'Survived'] = 1\n",
    "test['Survived'] = test['Survived'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0920160",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test[['PassengerId', 'Survived']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5184b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test[['PassengerId', 'Survived']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f35c9",
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}